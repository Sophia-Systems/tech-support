# CLAUDE.md — Project Context for AI Assistants

## What This Is
Production RAG customer service bot framework. Answers technical questions about industrial equipment (Dexter dryers, dry cleaning machines) using ingested documentation. Never hallucates — prefers "I don't know" or human escalation.

## Current State (2026-02-06)
- **Phases 1-5 complete** — full stack implemented and running locally
- **6 commits** on `main` branch, all atomic per phase
- **Local dev running**: API on :8000, frontend on :5176 (5173 was taken), Postgres+Redis via Docker Compose
- **2 sample docs ingested**: dryer-maintenance.md (5 chunks), dryer-troubleshooting.md (7 chunks)
- **End-to-end verified**: ANSWER, OFF_TOPIC tiers tested and working through the UI

## Architecture Quick Reference
```
Frontend (React+Vite :5176) → API (FastAPI :8000) → RAG Pipeline → Postgres (pgvector + FTS) + Redis
```

### Key Directories
- `backend/app/` — FastAPI application
- `backend/app/providers/` — Swappable provider implementations (LLM, embeddings, vectorstore, reranker, keyword search)
- `backend/app/services/` — Business logic (rag_pipeline, confidence, persona, escalation, session_manager, ingestion_pipeline)
- `backend/app/ingestion/` — Document loaders, chunkers, processors
- `backend/config/` — YAML config (retrieval tuning, persona templates)
- `frontend/src/` — React app with chat UI
- `data/sample-docs/` — Test markdown docs
- `data/manuals/` — Real PDF manuals (gitignored)

### Config
- `.env` at project root — loaded by pydantic-settings via `env_file` parameter
- `backend/config/default.yaml` — retrieval tuning, confidence thresholds, persona config
- `backend/config/persona/default.yaml` — Jinja2 system prompt template

## Current Configuration
- **LLM**: `anthropic/claude-sonnet-4-20250514` via LiteLLM
- **Embeddings**: `sentence-transformers/all-MiniLM-L6-v2` (local, 384 dims)
- **Reranker**: `cross-encoder/ms-marco-MiniLM-L-6-v2` (local)
- **Vector store**: pgvector (384-dim column)
- **Keyword search**: PostgreSQL FTS (tsvector)

## Known Issues / TODOs
1. **Source score display**: Cross-encoder outputs raw logits (unbounded), displayed as percentages in UI (shows 1052%, 584%). Need to normalize scores to 0-1 range before sending to frontend.
2. **PDF ingestion**: `unstructured[pdf]` moved to optional dep (`pip install .[pdf]`). Not tested yet — have real PDFs in `data/manuals/`.
3. **LangFuse tracing**: Wired in config but not yet decorating pipeline stages.
4. **Alembic migration**: Uses `001_initial_schema.py` (manual, not autogenerated). Vector dim hardcoded to 384.
5. **Frontend Vite proxy**: Configured for :5173 → :8000, works regardless of actual Vite port since proxy target is absolute.
6. **No production Dockerfiles tested** — only local dev verified.

## Development Commands
```bash
make dev-up          # Start postgres + redis
make dev-api         # Start API with hot reload
make dev-frontend    # Start Vite dev server (or cd frontend && npm run dev)
make migrate         # Run alembic migrations
make test            # Run pytest
make lint            # Run ruff

# Ingest sample docs
cd backend && uv run python scripts/ingest_sample_docs.py
```

## Gotchas Discovered During Development
- **asyncpg + PostgreSQL casts**: Can't use `::type` syntax in SQLAlchemy `text()` with asyncpg — conflicts with `:param` binding. Use `CAST(x AS type)` instead.
- **pydantic-settings .env loading**: Each settings class needs `env_file=path` explicitly; pydantic-settings doesn't auto-discover .env from parent dirs.
- **hatchling build**: Needs `[tool.hatch.build.targets.wheel] packages = ["app"]` in pyproject.toml since package name doesn't match directory.
- **Cross-encoder scores**: `cross-encoder/ms-marco-MiniLM-L-6-v2` outputs unbounded logits, not 0-1 probabilities. Higher = more relevant but values can be >1 or negative.
